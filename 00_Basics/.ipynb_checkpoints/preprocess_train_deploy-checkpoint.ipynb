{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data spec\n",
    "* MNIST dataset : 70,000 * 785(1 label + 28 * 28 data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle, gzip\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    args, _ = argparse.ArgumentParser().parse_known_args()\n",
    "    \n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'raw_data.csv')\n",
    "    \n",
    "    raw_data = np.loadtxt(input_data_path, delimiter=',')\n",
    "    \n",
    "    data = raw_data[:, 1:]\n",
    "    label = raw_data[:, 0]\n",
    "    \n",
    "    split_ratio = 0.2\n",
    "    train_data, test_data, train_label, test_label = train_test_split(data, label, test_size = 0.2)\n",
    "    \n",
    "    output_path = os.path.join('/opt/ml/processing/output', 'dataset.pkl.gz')\n",
    "    \n",
    "    with gzip.open(output_path, 'wb') as f:\n",
    "        pickle.dump((train_data, train_label, test_data, test_label), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)\n",
    "\n",
    "input_data = 's3://your-bucket/your-prefix/raw_data.csv'\n",
    "\n",
    "sklearn_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                          source=input_data,\n",
    "                          destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(\n",
    "                          output_name='output',\n",
    "                          source='/opt/ml/processing/output',\n",
    "                          destination='s3://your-bucket/your-prefix')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnist.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os, time\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip, pickle\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir', type=str)\n",
    "    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    input_path = os.path.join(args.train, 'dataset.pkl.gz')\n",
    "    with gzip.open(input_path, 'rb') as f:\n",
    "        train_data, train_label, test_data, test_label = pickle.load(f)\n",
    "        \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_data, train_label, epochs=3, verbose=2)\n",
    "    test_loss, test_acc = model.evaluate(test_data, test_label, verbose=0)\n",
    "    print('Test Average loss: {}, Test Accuracy: {};'.format(test_loss, test_acc))\n",
    "    \n",
    "    model.save(os.path.join(args.sm_model_dir, '000000001'), 'my_model.h5')\n",
    "    \n",
    "    print('training time: {}'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type='ml.m5.xlarge',\n",
    "                             train_use_spot_instances = True,\n",
    "                             train_max_run = 600, \n",
    "                             train_max_wait = 1200,\n",
    "                             framework_version='2.1.0',\n",
    "                             py_version='py3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mnist_estimator.fit('s3://your-bucket/your-prefix/dataset.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mnist_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_sample = np.loadtxt('../00_Basics/test_sample.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_sample[:10, 1:]\n",
    "test_label = test_sample[:10, 0]\n",
    "predictions = predictor.predict(test_data)\n",
    "for i in range(0, 10):\n",
    "    prediction = np.argmax(predictions['predictions'][i])\n",
    "    label = test_label[i]\n",
    "    print('prediction is {}, label is {}, matched: {}'.format(prediction, label, prediction == label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
