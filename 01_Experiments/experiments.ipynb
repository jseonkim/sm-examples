{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install SageMaker Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import time\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "role = get_execution_role()\n",
    "input_uri = 's3://sagemaker-us-east-1-233037139193/mbp3/dataset/dataset.pkl.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_experiment = Experiment.create(experiment_name=\"my-private-exp3\",\n",
    "                                  description=\"It's private\")\n",
    "print(my_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnist_softmax.py\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os, time\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip, pickle\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    start = time.time()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir', type=str)\n",
    "    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))  \n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    input_path = os.path.join(args.train, 'dataset.pkl.gz')\n",
    "    with gzip.open(input_path, 'rb') as f:\n",
    "        train_data, train_label, test_data, test_label = pickle.load(f)\n",
    "        \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_data, train_label, epochs=3, verbose=2)\n",
    "    model.evaluate(test_data, test_label, verbose=0)\n",
    "    \n",
    "    model.save(os.path.join(args.sm_model_dir, '000000001'), 'my_model.h5')\n",
    "        \n",
    "    print(\"training time: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnist_simple_nn.py\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os, time\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip, pickle\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    start = time.time()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir', type=str)\n",
    "    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))  \n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    input_path = os.path.join(args.train, 'dataset.pkl.gz')\n",
    "    with gzip.open(input_path, 'rb') as f:\n",
    "        train_data, train_label, test_data, test_label = pickle.load(f)\n",
    "        \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_data, train_label, epochs=3, verbose=2)\n",
    "    model.evaluate(test_data, test_label, verbose=0)\n",
    "    \n",
    "    model.save(os.path.join(args.sm_model_dir, '000000001'), 'my_model.h5')\n",
    "        \n",
    "    print(\"training time: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial creation & run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = f\"simple-nn-64-{int(time.time())}\"\n",
    "trial = Trial.create(trial_name=trial_name, \n",
    "                     experiment_name=my_experiment.experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='mnist_simple_nn.py',\n",
    "                       role=role,\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type='ml.m5.xlarge',\n",
    "                       train_use_spot_instances = True,\n",
    "                       train_max_run = 600,\n",
    "                       train_max_wait = 1200,                     \n",
    "                       framework_version='2.1.0',\n",
    "                       py_version='py3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs=input_uri,\n",
    "              job_name=trial_name,\n",
    "              experiment_config={\n",
    "                  \"TrialName\": trial.trial_name,\n",
    "                  \"TrialComponentDisplayName\": \"Training\"\n",
    "              })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression = {\n",
    "    \"Filters\":[\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "#experiment_name = \"my-private-exp3\"\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    experiment_name=my_experiment.experiment_name,\n",
    "    #experiment_name=experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.accuracy_EVAL.max\",\n",
    "    sort_order=\"Descending\",\n",
    "    metric_names=['accuracy_EVAL', 'loss_EVAL'],\n",
    "    parameter_names=['SageMaker.InstanceType', 'sagemaker_program']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "debug_trial = create_trial(estimator.latest_job_debugger_artifacts_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_trial.tensor_names(collection=\"losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import re\n",
    "\n",
    "plt.figure(\n",
    "    num=1, figsize=(8, 8), dpi=80,\n",
    "    facecolor='w', edgecolor='k')\n",
    "\n",
    "tensor = debug_trial.tensor('loss')\n",
    "steps = tensor.steps()\n",
    "data = [tensor.value(s) for s in steps]\n",
    "\n",
    "plt.plot(steps, data, label='Loss')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.04,1), loc='upper left')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter trial loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnist_simple_nn_h.py\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os, time\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip, pickle\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    start = time.time()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir', type=str)\n",
    "    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "    parser.add_argument('--number-of-nodes', type=int, default=128) # parameterize\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    input_path = os.path.join(args.train, 'dataset.pkl.gz')\n",
    "    with gzip.open(input_path, 'rb') as f:\n",
    "        train_data, train_label, test_data, test_label = pickle.load(f)\n",
    "        \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(args.number_of_nodes, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_data, train_label, epochs=3, verbose=2)\n",
    "    model.evaluate(test_data, test_label, verbose=0)\n",
    "    \n",
    "    model.save(os.path.join(args.sm_model_dir, '000000001'), 'my_model.h5')\n",
    "        \n",
    "    print(\"training time: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, number_of_nodes in enumerate([32, 128]):\n",
    "    trial_name = f\"simple-nn-{number_of_nodes}-{int(time.time())}\"\n",
    "    trial = Trial.create(trial_name=trial_name, \n",
    "                         experiment_name=my_experiment.experiment_name)\n",
    "    \n",
    "    estimator = TensorFlow(entry_point='mnist_simple_nn_h.py',\n",
    "                           role=role,\n",
    "                           train_instance_count=1,\n",
    "                           train_instance_type='ml.m5.xlarge',\n",
    "                           hyperparameters={\n",
    "                               'number_of_nodes': number_of_nodes\n",
    "                           },\n",
    "                           train_use_spot_instances = True,\n",
    "                           train_max_run = 600,\n",
    "                           train_max_wait = 1200,\n",
    "                           framework_version='2.1.0',\n",
    "                           py_version='py3')\n",
    "                           \n",
    "    estimator.fit(inputs=input_uri,\n",
    "                  job_name=trial_name,\n",
    "                  experiment_config={\n",
    "                      \"TrialName\": trial.trial_name,\n",
    "                      \"TrialComponentDisplayName\": \"Training\"\n",
    "                  })\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_analytics2 = ExperimentAnalytics(\n",
    "    experiment_name=my_experiment.experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.accuracy_EVAL.max\",\n",
    "    sort_order=\"Descending\",\n",
    "    metric_names=['accuracy_EVAL', 'loss_EVAL'],\n",
    "    parameter_names=['SageMaker.InstanceType', 'sagemaker_program']\n",
    ")\n",
    "trial_analytics2.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_estimator = TensorFlow(entry_point='mnist_simple_nn_h.py',\n",
    "                           role=role,\n",
    "                           train_instance_count=1,\n",
    "                           train_instance_type='ml.m5.xlarge',\n",
    "                           train_use_spot_instances = True,\n",
    "                           train_max_run = 600, \n",
    "                           train_max_wait = 1200,\n",
    "                           framework_version='2.1.0',\n",
    "                           py_version='py3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, HyperparameterTuner\n",
    "\n",
    "tuner = HyperparameterTuner(estimator=hpo_estimator,\n",
    "                            objective_metric_name='loss',\n",
    "                            objective_type='Minimize',\n",
    "                            hyperparameter_ranges={\n",
    "                                'number_of_nodes': IntegerParameter(32, 128)\n",
    "                            },\n",
    "                            metric_definitions=[{\n",
    "                                'Name': 'loss',\n",
    "                                'Regex': 'loss: ([0-9\\\\.]+)'\n",
    "                            }],\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=2, # What number is the best? \n",
    "                            early_stopping_type='Auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "hpo_job_name='hpo-job-{}'.format(timestamp)\n",
    "tuner.fit(inputs=input_uri, job_name=hpo_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client('sagemaker')\n",
    "#hpo_job_name='hpo-job-2020-07-01-05-41-03'\n",
    "# run this cell to check current status of hyperparameter tuning job\n",
    "tuning_job_result = sm_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=hpo_job_name)\n",
    "\n",
    "status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "if status != 'Completed':\n",
    "    print('Reminder: the tuning job has not been completed.')\n",
    "    \n",
    "job_count = tuning_job_result['TrainingJobStatusCounters']['Completed']\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "\n",
    "is_minimize = (tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['Type'] != 'Maximize')\n",
    "objective_name = tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['MetricName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sagemaker import HyperparameterTuningJobAnalytics\n",
    "\n",
    "tuner_result = HyperparameterTuningJobAnalytics(hpo_job_name)\n",
    "\n",
    "full_df = tuner_result.dataframe()\n",
    "\n",
    "if len(full_df) > 0:\n",
    "    df = full_df[full_df['FinalObjectiveValue'] > -float('inf')]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values('FinalObjectiveValue', ascending=is_minimize)\n",
    "        print(\"Number of training jobs with valid objective: %d\" % len(df))\n",
    "        print({\"lowest\":min(df['FinalObjectiveValue']),\"highest\": max(df['FinalObjectiveValue'])})\n",
    "        pd.set_option('display.max_colwidth', -1)  # Don't truncate TrainingJobName        \n",
    "    else:\n",
    "        print(\"No training jobs have reported valid results yet.\")\n",
    "        \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for showing the HPO results by time\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "bokeh.io.output_notebook()\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "class HoverHelper():\n",
    "\n",
    "    def __init__(self, tuning_analytics):\n",
    "        self.tuner = tuning_analytics\n",
    "\n",
    "    def hovertool(self):\n",
    "        tooltips = [\n",
    "            (\"FinalObjectiveValue\", \"@FinalObjectiveValue\"),\n",
    "            (\"TrainingJobName\", \"@TrainingJobName\"),\n",
    "        ]\n",
    "        for k in self.tuner.tuning_ranges.keys():\n",
    "            tooltips.append( (k, \"@{%s}\" % k) )\n",
    "\n",
    "        ht = HoverTool(tooltips=tooltips)\n",
    "        return ht\n",
    "\n",
    "    def tools(self, standard_tools='pan,crosshair,wheel_zoom,zoom_in,zoom_out,undo,reset'):\n",
    "        return [self.hovertool(), standard_tools]\n",
    "\n",
    "hover = HoverHelper(tuner_result)\n",
    "\n",
    "p = figure(plot_width=900, plot_height=400, tools=hover.tools(), x_axis_type='datetime')\n",
    "p.circle(source=df, x='TrainingStartTime', y='FinalObjectiveValue')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = tuner_result.tuning_ranges\n",
    "figures = []\n",
    "for hp_name, hp_range in ranges.items():\n",
    "    categorical_args = {}\n",
    "    if hp_range.get('Values'):\n",
    "        # This is marked as categorical.  Check if all options are actually numbers.\n",
    "        def is_num(x):\n",
    "            try:\n",
    "                float(x)\n",
    "                return 1\n",
    "            except:\n",
    "                return 0           \n",
    "        vals = hp_range['Values']\n",
    "        if sum([is_num(x) for x in vals]) == len(vals):\n",
    "            # Bokeh has issues plotting a \"categorical\" range that's actually numeric, so plot as numeric\n",
    "            print(\"Hyperparameter %s is tuned as categorical, but all values are numeric\" % hp_name)\n",
    "        else:\n",
    "            # Set up extra options for plotting categoricals.  A bit tricky when they're actually numbers.\n",
    "            categorical_args['x_range'] = vals\n",
    "\n",
    "    # Now plot it\n",
    "    p = figure(plot_width=500, plot_height=500, \n",
    "               title=\"Objective vs %s\" % hp_name,\n",
    "               tools=hover.tools(),\n",
    "               x_axis_label=hp_name, y_axis_label=objective_name,\n",
    "               **categorical_args)\n",
    "    p.circle(source=df, x=hp_name, y='FinalObjectiveValue')\n",
    "    figures.append(p)\n",
    "show(bokeh.layouts.Column(*figures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Experiments Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"our-experiment-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_estimator = TensorFlow(entry_point='mnist_simple_nn_h.py',\n",
    "                             role=role,\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type='ml.m5.xlarge',\n",
    "                             hyperparameters={\n",
    "                                 'number_of_nodes': 83\n",
    "                             },\n",
    "                             metric_definitions=[\n",
    "                                 {'Name': 'Training:seconds', 'Regex': 'training time: ([0-9\\\\.]+)'}\n",
    "                             ],\n",
    "                             train_use_spot_instances = True,\n",
    "                             train_max_run = 600,\n",
    "                             train_max_wait = 1200,\n",
    "                             framework_version='2.1.0',\n",
    "                             py_version='py3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trial_name = f\"simple-nn-83-{int(time.time())}\"\n",
    "my_trial = Trial.create(trial_name=my_trial_name, \n",
    "                        experiment_name=experiment_name)\n",
    "final_estimator.fit(inputs=input_uri,\n",
    "                    job_name=my_trial_name,\n",
    "                    experiment_config={\n",
    "                        \"TrialName\": my_trial.trial_name,\n",
    "                        \"TrialComponentDisplayName\": \"Training\"\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trial_analytics = ExperimentAnalytics(\n",
    "    experiment_name=experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.accuracy_EVAL.max\",\n",
    "    sort_order=\"Descending\",\n",
    "    metric_names=['accuracy_EVAL', 'loss_EVAL', 'Training:seconds'],\n",
    "    parameter_names=['SageMaker.InstanceType', 'sagemaker_program']\n",
    ")\n",
    "all_trial_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
